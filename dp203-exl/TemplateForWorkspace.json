{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "dp203-exl"
		},
		"dp203-exl-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'dp203-exl-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:dp203-exl.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"synapse5hueysr-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapse5hueysr-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapse5hueysr.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"dp203-exl-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dp203dushyantdatalake.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Workflow_update_sales')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Create table",
						"type": "Script",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "dp203-exl-WorkspaceDefaultSqlServer",
							"type": "LinkedServiceReference",
							"parameters": {
								"DBName": "Sales_data_transformed"
							}
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "IF OBJECT_ID('dbo.Sales_data', 'U') IS NOT NULL\n    DROP TABLE dbo.Sales_data;\n\n\nCREATE TABLE [dbo].[Sales_data]\n(\n    sale_id VARCHAR(255) NULL,\n    product_id VARCHAR(255) NULL,\n    product_name VARCHAR(255) NULL,\n    quantity INT NULL,\n    price FLOAT NULL,\n    total_amount FLOAT NULL,\n    sale_date DATETIME NULL,\n    region VARCHAR(255) NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH ([sale_date])\n)\n"
								}
							],
							"scriptBlockExecutionTimeout": "02:00:00"
						}
					},
					{
						"name": "Combine data",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Create table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Combine data",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "dp203exl",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false
							},
							"driverSize": "Small"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dp203-exl-WorkspaceDefaultSqlServer')]",
				"[concat(variables('workspaceId'), '/notebooks/Combine data')]",
				"[concat(variables('workspaceId'), '/bigDataPools/dp203exl')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataset')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "dp203-exl-WorkspaceDefaultSqlServer",
					"type": "LinkedServiceReference",
					"parameters": {
						"DBName": "Sales_data_transformed"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"tableName": "dbo.Sales_d"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dp203-exl-WorkspaceDefaultSqlServer')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataset1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "dp203-exl-WorkspaceDefaultSqlServer",
					"type": "LinkedServiceReference",
					"parameters": {
						"DBName": "Sales_data_transformed"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"tableName": "dbo.Sales_d"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dp203-exl-WorkspaceDefaultSqlServer')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataset2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "dp203-exl-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Event_Hub_Data",
						"fileSystem": "data"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dp203-exl-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dp203-exl-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('dp203-exl-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dp203-exl-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dp203-exl-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse5hueysr-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapse5hueysr-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Comabine all csv')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://dp203dushyantdatalake.dfs.core.windows.net/data/sales_data/poscsv/**',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Combine parquet')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalake5hueysr.dfs.core.windows.net/files/sales_data/posparquet/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [sale_id]\n,[product_id]\n,[product_name]\n,[quantity]\n,[price]\n,[total_amount]\n,[sale_date]\n,[region]\n FROM [dbo].[Sales_data]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales_data_transformed",
						"poolName": "Sales_data_transformed"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (1000) [sale_id]\n,[product_id]\n,[product_name]\n,[quantity]\n,[price]\n,[total_amount]\n,[sale_date]\n,[region]\n FROM [dbo].[Sales_data]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales_data_transformed",
						"poolName": "Sales_data_transformed"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ext_database')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "DROP TABLE [dbo].[Sales_data]\nGO\n\n\nCREATE TABLE [dbo].[Sales_data]\n(\n    [sale_id] VARCHAR(255) NULL,\n    [product_id] VARCHAR(255) NULL,\n    [product_name] VARCHAR(255) NULL,\n    [quantity] INT NULL,\n    [price] FLOAT NULL,\n    [total_amount] FLOAT NULL,\n    [sale_date] DATETIME2  NULL,\n    [region] VARCHAR(255) NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH ([sale_date])\n)\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales_data_transformed",
						"poolName": "Sales_data_transformed"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Combine data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "dp203exl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e1ea248a-222c-4e25-9f22-8950dd7d0686"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/2b95d830-fc0d-4d49-82a7-83baa7f46a17/resourceGroups/dp203-exl/providers/Microsoft.Synapse/workspaces/dp203-exl/bigDataPools/dp203exl",
						"name": "dp203exl",
						"type": "Spark",
						"endpoint": "https://dp203-exl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dp203exl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# **Read input file **"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\n",
							"from pyspark.sql.functions import col, to_date , to_timestamp\n",
							"from pyspark.sql.types import IntegerType, DoubleType\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"csv_df  = spark.read.option(\"recursiveFileLookup\",\"true\").csv(\"abfss://data@dp203dushyantdatalake.dfs.core.windows.net/sales_data/poscsv/*\",header=True)\n",
							"\n",
							"parquet_df = spark.read \\\n",
							"    .option(\"recursiveFileLookup\", \"true\").parquet(\"abfss://data@dp203dushyantdatalake.dfs.core.windows.net/sales_data/posparquet/\")\n",
							"\n",
							"csv_df.count() "
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"csv_df = csv_df.union(parquet_df)\r\n",
							"csv_df.count() "
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Drop na\r\n",
							"csv_df = csv_df.dropna()\r\n",
							"\r\n",
							"# Drop duplicates\r\n",
							"csv_df = csv_df.dropDuplicates()\r\n",
							"\r\n",
							"print(csv_df.dtypes)\r\n",
							"csv_df.count() "
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert 'quantity' column to IntegerType\r\n",
							"csv_df = csv_df.withColumn(\"quantity\", col(\"quantity\").cast(IntegerType()))\r\n",
							"\r\n",
							"# Convert 'price' column to DoubleType\r\n",
							"csv_df = csv_df.withColumn(\"price\", col(\"price\").cast(DoubleType()))\r\n",
							"\r\n",
							"# Convert 'total_amount' column to DoubleType\r\n",
							"csv_df = csv_df.withColumn(\"total_amount\", col(\"total_amount\").cast(DoubleType()))\r\n",
							"\r\n",
							"# Cast the sale_date column to TIMESTAMP\r\n",
							"csv_df = csv_df.withColumn(\"sale_date\", col(\"sale_date\").cast(\"TIMESTAMP\"))\r\n",
							"\r\n",
							"# Show the DataFrame and schema to verify changes\r\n",
							"csv_df.printSchema()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example transformation: Calculate total sales by region\r\n",
							"sales_df = csv_df.groupBy(\"region\",\"product_name\").agg({\"total_amount\": \"sum\"})\r\n",
							"sales_df.show()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define the JDBC URL and connection properties\r\n",
							"jdbc_url = \"jdbc:sqlserver://dp203-exl.sql.azuresynapse.net:1433;database=Sales_data_transformed;user=sqladminuser123@dp203-exl;password=Dushyant@099;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\r\n",
							"jdbc_properties = {\r\n",
							"    \"user\": \"sqladminuser123\",\r\n",
							"    \"password\": \"Dushyant@099\",\r\n",
							"    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
							"}\r\n",
							"\r\n",
							"\r\n",
							"# Write the DataFrame to the SQL Pool table\r\n",
							"try:\r\n",
							"    csv_df.write \\\r\n",
							"        .mode(\"append\") \\\r\n",
							"        .jdbc(url=jdbc_url, table=\"Sales_data\", properties=jdbc_properties)\r\n",
							"    print(\"DataFrame successfully written to Sales_data\")\r\n",
							"except Exception as e:\r\n",
							"    print(\"An error occurred while writing the DataFrame to SQL Pool:\", str(e))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"json_df = spark.read.option(\"recursiveFileLookup\", \"true\").json(\"abfss://data@dp203dushyantdatalake.dfs.core.windows.net/sales_data/onlinetransaction/\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Display the schema\r\n",
							"json_df.printSchema()\r\n",
							"\r\n",
							"# Show a sample of the data\r\n",
							"json_df.show()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Fill missing values\r\n",
							"cleaned_df = json_df \\\r\n",
							"    .fillna({\r\n",
							"        \"price\": 0.0,\r\n",
							"        \"quantity\": 0.0,\r\n",
							"        \"total_amount\": 0.0,\r\n",
							"        \"product_id\": \"unknown\",\r\n",
							"        \"product_name\": \"unknown\",\r\n",
							"        \"region\": \"unknown\",\r\n",
							"        \"sale_date\": \"unknown\",\r\n",
							"        \"sale_id\": \"unknown\"\r\n",
							"    })\r\n",
							"\r\n",
							"# Alternatively, drop rows with missing values in specific columns\r\n",
							"cleaned_df = json_df \\\r\n",
							"    .dropna(subset=[\"price\", \"quantity\", \"total_amount\", \"product_id\", \"product_name\", \"region\", \"sale_date\", \"sale_id\"])\r\n",
							"\r\n",
							"# Optionally, you can use different strategies for different columns\r\n",
							"# Example: Fill missing `sale_date` with a placeholder and drop rows with missing numeric columns\r\n",
							"cleaned_df = json_df \\\r\n",
							"    .fillna({\r\n",
							"        \"product_id\": \"unknown\",\r\n",
							"        \"product_name\": \"unknown\",\r\n",
							"        \"region\": \"unknown\",\r\n",
							"        \"sale_date\": \"unknown\",\r\n",
							"        \"sale_id\": \"unknown\"\r\n",
							"    }) \\\r\n",
							"    .dropna(subset=[\"price\", \"quantity\", \"total_amount\"])\r\n",
							"\r\n",
							"# Show the cleaned DataFrame\r\n",
							"cleaned_df.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Save cleaned data to Azure Data Lake Gen 2 in Parquet format\r\n",
							"cleaned_df.write.mode(\"overwrite\").json(\"abfss://data@dp203dushyantdatalake.dfs.core.windows.net/Event_Hub_Data/\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "dp203exl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b8d860df-6c90-41d1-b69d-96d6b7862afb"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/2b95d830-fc0d-4d49-82a7-83baa7f46a17/resourceGroups/dp203-exl/providers/Microsoft.Synapse/workspaces/dp203-exl/bigDataPools/dp203exl",
						"name": "dp203exl",
						"type": "Spark",
						"endpoint": "https://dp203-exl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dp203exl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure.eventhub"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"#!/usr/bin/env python\r\n",
							"\r\n",
							"# --------------------------------------------------------------------------------------------\r\n",
							"# Copyright (c) Microsoft Corporation. All rights reserved.\r\n",
							"# Licensed under the MIT License. See License.txt in the project root for license information.\r\n",
							"# --------------------------------------------------------------------------------------------\r\n",
							"\r\n",
							"\"\"\"\r\n",
							"Examples to show how to create EventHubProducerClient/EventHubConsumerClient.\r\n",
							"\"\"\"\r\n",
							"import os\r\n",
							"from azure.eventhub import (\r\n",
							"    EventHubProducerClient,\r\n",
							"    EventHubConsumerClient,\r\n",
							"    TransportType,\r\n",
							"    EventHubSharedKeyCredential\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"CONNECTION_STRING = os.environ['']\r\n",
							"FULLY_QUALIFIED_NAMESPACE = os.environ['eventhubjsonsales ']\r\n",
							"EVENTHUB_NAME = os.environ['eventhubjsonsales.servicebus.windows.net']\r\n",
							"SAS_POLICY = os.environ['EVENT_HUB_SAS_POLICY']\r\n",
							"SAS_KEY = os.environ[]\r\n",
							"CONSUMER_GROUP = \"$Default\"\r\n",
							"\r\n",
							"\r\n",
							"def create_producer_client():\r\n",
							"    print('Examples showing how to create producer client.')\r\n",
							"\r\n",
							"    # Create producer client from connection string.\r\n",
							"\r\n",
							"    producer_client = EventHubProducerClient.from_connection_string(\r\n",
							"        conn_str=CONNECTION_STRING  # connection string contains EventHub name.\r\n",
							"    )\r\n",
							"\r\n",
							"    # Illustration of commonly used parameters.\r\n",
							"    producer_client = EventHubProducerClient.from_connection_string(\r\n",
							"        conn_str=CONNECTION_STRING,\r\n",
							"        eventhub_name=EVENTHUB_NAME,  # EventHub name should be specified if it doesn't show up in connection string.\r\n",
							"        logging_enable=False,  # To enable network tracing log, set logging_enable to True.\r\n",
							"        retry_total=3,  # Retry up to 3 times to re-do failed operations.\r\n",
							"        transport_type=TransportType.Amqp  # Use Amqp as the underlying transport protocol.\r\n",
							"    )\r\n",
							"\r\n",
							"    # Create producer client from constructor.\r\n",
							"\r\n",
							"    producer_client = EventHubProducerClient(\r\n",
							"        fully_qualified_namespace=FULLY_QUALIFIED_NAMESPACE,\r\n",
							"        eventhub_name=EVENTHUB_NAME,\r\n",
							"        credential=EventHubSharedKeyCredential(\r\n",
							"            policy=SAS_POLICY,\r\n",
							"            key=SAS_KEY\r\n",
							"        ),\r\n",
							"        logging_enable=False,  # To enable network tracing log, set logging_enable to True.\r\n",
							"        retry_total=3,  # Retry up to 3 times to re-do failed operations.\r\n",
							"        transport_type=TransportType.Amqp  # Use Amqp as the underlying transport protocol.\r\n",
							"    )\r\n",
							"\r\n",
							"    print(\"Calling producer client get eventhub properties:\", producer_client.get_eventhub_properties())\r\n",
							"\r\n",
							"\r\n",
							"def create_consumer_client():\r\n",
							"    print('Examples showing how to create consumer client.')\r\n",
							"\r\n",
							"    # Create consumer client from connection string.\r\n",
							"\r\n",
							"    consumer_client = EventHubConsumerClient.from_connection_string(\r\n",
							"        conn_str=CONNECTION_STRING,  # connection string contains EventHub name.\r\n",
							"        consumer_group=CONSUMER_GROUP\r\n",
							"    )\r\n",
							"\r\n",
							"    # Illustration of commonly used parameters.\r\n",
							"    consumer_client = EventHubConsumerClient.from_connection_string(\r\n",
							"        conn_str=CONNECTION_STRING,\r\n",
							"        consumer_group=CONSUMER_GROUP,\r\n",
							"        eventhub_name=EVENTHUB_NAME,  # EventHub name should be specified if it doesn't show up in connection string.\r\n",
							"        logging_enable=False,  # To enable network tracing log, set logging_enable to True.\r\n",
							"        retry_total=3,  # Retry up to 3 times to re-do failed operations.\r\n",
							"        transport_type=TransportType.Amqp  # Use Amqp as the underlying transport protocol.\r\n",
							"    )\r\n",
							"\r\n",
							"    # Create consumer client from constructor.\r\n",
							"\r\n",
							"    consumer_client = EventHubConsumerClient(\r\n",
							"        fully_qualified_namespace=FULLY_QUALIFIED_NAMESPACE,\r\n",
							"        eventhub_name=EVENTHUB_NAME,\r\n",
							"        consumer_group=CONSUMER_GROUP,\r\n",
							"        credential=EventHubSharedKeyCredential(\r\n",
							"            policy=SAS_POLICY,\r\n",
							"            key=SAS_KEY\r\n",
							"        ),\r\n",
							"        logging_enable=False,  # To enable network tracing log, set logging_enable to True.\r\n",
							"        retry_total=3,  # Retry up to 3 times to re-do failed operations.\r\n",
							"        transport_type=TransportType.Amqp  # Use Amqp as the underlying transport protocol.\r\n",
							"    )\r\n",
							"\r\n",
							"    print(\"Calling consumer client get eventhub properties:\", consumer_client.get_eventhub_properties())\r\n",
							"\r\n",
							"\r\n",
							"create_producer_client()\r\n",
							"create_consumer_client()"
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Send_data_eventhub')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "dp203exl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ce1466e6-b173-49b6-aac8-489e9e39e53f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/2b95d830-fc0d-4d49-82a7-83baa7f46a17/resourceGroups/dp203-exl/providers/Microsoft.Synapse/workspaces/dp203-exl/bigDataPools/dp203exl",
						"name": "dp203exl",
						"type": "Spark",
						"endpoint": "https://dp203-exl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dp203exl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-eventhub"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define the path to the JSON files in ADLS\r\n",
							"input_path = \"abfss://data@dp203dushyantdatalake.dfs.core.windows.net/Event_Hub_Data/\"\r\n",
							"\r\n",
							"# Read the JSON files into a DataFrame\r\n",
							"df = spark.read.json(input_path)\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define input and output paths\r\n",
							"input_path = \"abfss://data@dp203dushyantdatalake.dfs.core.windows.net/Event_Hub_Data/\"\r\n",
							"output_path = \"abfss://data@dp203dushyantdatalake.dfs.core.windows.net/Event_Hub_Data_Cleaned/\"\r\n",
							"\r\n",
							"# Read JSON files into DataFrame\r\n",
							"df = spark.read.json(input_path)\r\n",
							"\r\n",
							"# Data cleaning\r\n",
							"df_cleaned = (df\r\n",
							"    .drop(\"sale_id\")\r\n",
							"    .fillna({\r\n",
							"        \"product_id\": \"unknown\",\r\n",
							"        \"product_name\": \"unknown\",\r\n",
							"        \"quantity\": 0,\r\n",
							"        \"total_amount\": 0.0\r\n",
							"    })\r\n",
							"    .filter(col(\"sale_date\").isNotNull())\r\n",
							"    .filter(col(\"price\").isNotNull())\r\n",
							"    .filter(col(\"region\").isNotNull())\r\n",
							"    .filter(col(\"product_id\").isNotNull())\r\n",
							"    .filter(col(\"product_name\").isNotNull())\r\n",
							")\r\n",
							"\r\n",
							"# Write cleaned DataFrame back to ADLS\r\n",
							"df_cleaned.write.mode(\"overwrite\").json(output_path)\r\n",
							"\r\n",
							"print(\"JSON files cleaned and written to:\", output_path)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"\r\n",
							"# Define input path and read JSON files\r\n",
							"input_path = \"abfss://data@dp203dushyantdatalake.dfs.core.windows.net/Event_Hub_Data/\"\r\n",
							"df = spark.read.json(input_path)\r\n",
							"\r\n",
							"# Data cleaning\r\n",
							"df_cleaned = (df\r\n",
							"    .drop(\"sale_id\")\r\n",
							"    .fillna({\r\n",
							"        \"product_id\": \"unknown\",\r\n",
							"        \"product_name\": \"unknown\",\r\n",
							"        \"quantity\": 0,\r\n",
							"        \"total_amount\": 0.0\r\n",
							"    })\r\n",
							"    .filter(col(\"sale_date\").isNotNull())\r\n",
							"    .filter(col(\"price\").isNotNull())\r\n",
							"    .filter(col(\"region\").isNotNull())\r\n",
							"    .filter(col(\"product_id\").isNotNull())\r\n",
							"    .filter(col(\"product_name\").isNotNull())\r\n",
							")\r\n",
							"\r\n",
							"# Define Event Hub connection parameters\r\n",
							"event_hub_connection_string = (\r\n",
							"    \"\r\n",
							")\r\n",
							"\r\n",
							"# Define the DataFrame to send\r\n",
							"# Ensure DataFrame is in the correct format for Event Hub\r\n",
							"df_cleaned = df_cleaned.selectExpr(\"to_json(struct(*)) AS body\")\r\n",
							"\r\n",
							"# Send the DataFrame to Event Hub\r\n",
							"try:\r\n",
							"    df_cleaned.write \\\r\n",
							"        .format(\"eventhubs\") \\\r\n",
							"        .option(\"eventhubs.connectionString\", event_hub_connection_string) \\\r\n",
							"        .save()\r\n",
							"    print(\"DataFrame successfully sent to Event Hub.\")\r\n",
							"except Exception as e:\r\n",
							"    print(\"An error occurred while sending the DataFrame to Event Hub:\", str(e))"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sparkContext.setLogLevel(\"DEBUG\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 21
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dp203exl')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 7
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 4,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastasia"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sales_data_transformed')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastasia"
		}
	]
}